{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Head Estimation, Pose Conversion & Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Process Keypoints JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([0.16450216450216448, -0.4415584415584416, 0.303030303030303, -0.5930735930735931, -1.0, -0.5411255411255411, 0.1212121212121211, -0.2987012987012987, 0.48051948051948057, -0.5238095238095238, 0.48051948051948057, -0.5281385281385281, -0.37662337662337664, -0.35930735930735935, -0.1515151515151515, -0.11688311688311692, -1.0, -0.07359307359307354, 0.2554112554112553, 0.2683982683982684, 0.48484848484848486, 0.48484848484848486, 0.9610389610389611, 1.0], [-0.10822510822510822, -0.04761904761904767, -0.18614718614718617, 0.017316017316017396, -0.2727272727272727, -0.6580086580086579, -0.7012987012987013, -0.7012987012987013, -0.6320346320346321, -0.6190476190476191]), ([0.9293598233995586, 0.7792494481236203, 1.0, 0.739514348785872, 0.9426048565121412, 0.7924944812362031, 0.9293598233995586, 0.7880794701986755, -1.0, -1.0, -1.0, -1.0, -0.015452538631346546, -0.01103752759381893, 0.06843267108167761, 0.04635761589403975, 0.050772626931567366, 0.04635761589403975, 0.14348785871964687, 0.14790286975717448, -1.0, -1.0, -1.0, -1.0], [0.8498896247240617, 0.8587196467991169, 0.8366445916114791, 0.8763796909492274, 0.814569536423841, -0.08609271523178808, -0.09933774834437081, -0.09933774834437081, -0.09492273730684331, -0.09050772626931569]), ([0.23348017621145378, -0.2819383259911894, 0.35242290748898686, -0.4977973568281938, 0.2951541850220265, -0.07929515418502198, 0.22466960352422904, -0.21145374449339205, -1.0, -1.0, -1.0, -1.0, 0.017621145374449254, -0.048458149779735726, 0.4449339207048457, 0.4625550660792952, 0.8986784140969164, 0.5154185022026432, 0.9603524229074889, 1.0, -1.0, -1.0, -1.0, -1.0], [0.21145374449339216, 0.24229074889867852, 0.12775330396475781, 0.27312775330396466, -0.004405286343612369, -0.4008810572687225, -0.44493392070484583, -0.4757709251101322, -0.39207048458149785, -0.43171806167400884]), ([0.9632107023411371, 0.8929765886287626, 0.9899665551839465, -1.0, 1.0, -1.0, 0.9632107023411371, 0.9063545150501673, 0.9431438127090301, 0.9130434782608696, 0.9531772575250836, 0.9297658862876255, -0.5986622073578596, -0.5919732441471572, -0.5317725752508361, -1.0, -0.4682274247491639, -1.0, -0.46488294314381273, -0.46153846153846156, -0.3712374581939799, -0.3745819397993311, -0.26086956521739135, -0.2642140468227425], [0.9163879598662208, 0.9264214046822743, 0.9096989966555185, 0.939799331103679, 0.899665551839465, -0.6421404682274248, -0.6521739130434783, -0.6521739130434783, -0.6555183946488294, -0.6521739130434783]), ([0.5741935483870968, 0.3978494623655915, 0.6129032258064515, 0.33333333333333326, -1.0, 0.2172043010752689, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 0.7548387096774194, 0.7548387096774194, 0.9483870967741936, 0.8709677419354838, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0], [0.44946236559139785, 0.4623655913978495, 0.43225806451612914, 0.5053763440860215, 0.4107526881720429, 0.6516129032258065, 0.6301075268817204, 0.6344086021505377, 0.6344086021505377, 0.6516129032258065])]\n",
      "Number of extracted poses: 23713\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load person_keypoints_train2017.json and extract poses which have all of their body keypoints\n",
    "'''\n",
    "\"keypoints\": [\n",
    "                        \"nose\",\n",
    "                        \"left_eye\",\n",
    "                        \"right_eye\",\n",
    "                        \"left_ear\",\n",
    "                        \"right_ear\",\n",
    "                        \n",
    "                        \"left_shoulder\",\n",
    "                        \"right_shoulder\",\n",
    "                        \"left_elbow\",\n",
    "                        \"right_elbow\",\n",
    "                        \"left_wrist\",\n",
    "                        \"right_wrist\",\n",
    "\n",
    "                        \"left_hip\",\n",
    "                        \"right_hip\",\n",
    "                        \"left_knee\",\n",
    "                        \"right_knee\",\n",
    "                        \"left_ankle\",\n",
    "                        \"right_ankle\"\n",
    "                    ],\n",
    "'''\n",
    "\n",
    "def extract_poses(file_path, scale=2):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    poses_list = []\n",
    "\n",
    "    for annotation in data['annotations']:\n",
    "        visibility = annotation['keypoints'][2::3]\n",
    "\n",
    "        if 0 not in visibility[:5]:\n",
    "            keypoints_x = annotation['keypoints'][0::3]\n",
    "            keypoints_y = annotation['keypoints'][1::3]\n",
    "\n",
    "            max_x = max(keypoints_x)\n",
    "            min_x = min(keypoints_x)\n",
    "            max_y = max(keypoints_y)\n",
    "            min_y = min(keypoints_y)\n",
    "\n",
    "            height = max_y - min_y\n",
    "            width = max_x - min_x\n",
    "\n",
    "            if height > width:\n",
    "                keypoints_x = [((x - min_x) / height - 0.5) * scale for x in keypoints_x]\n",
    "                keypoints_y = [((y - min_y) / height - 0.5) * scale for y in keypoints_y]\n",
    "            else:\n",
    "                keypoints_x = [((x - min_x) / width - 0.5) * scale for x in keypoints_x]\n",
    "                keypoints_y = [((y - min_y) / width - 0.5) * scale for y in keypoints_y]\n",
    "\n",
    "            poses_list.append((keypoints_x[5:] + keypoints_y[5:], \n",
    "                                   keypoints_x[:5] + keypoints_y[:5]))\n",
    "\n",
    "    return poses_list\n",
    "\n",
    "# File path to the JSON file\n",
    "file_path = 'person_keypoints_train2017.json'\n",
    "\n",
    "# Extract keypoints\n",
    "extracted_poses = extract_poses(file_path)\n",
    "\n",
    "# Print the result\n",
    "result_len = len(extracted_poses)\n",
    "print(extracted_poses[:5])\n",
    "print(f\"Number of extracted poses: {result_len}\")\n",
    "\n",
    "split = 0.8\n",
    "\n",
    "# Save the result\n",
    "with open(\"train.json\", \"w\") as train:\n",
    "    json.dump(extracted_poses[:int(result_len * split)], train)\n",
    "\n",
    "with open(\"test.json\", \"w\") as test:\n",
    "    json.dump(extracted_poses[int(result_len * split):], test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS Support: True\n",
      "MPS Availability: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "print(f\"MPS Support: {torch.backends.mps.is_built()}\")\n",
    "print(f\"MPS Availability: {torch.backends.mps.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "device = None\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataloader and Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 0.8\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        train_path = data_dir + \"/train.json\"\n",
    "\n",
    "        with open(train_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            data_len = len(data)\n",
    "            self.data = data[:int(data_len * split)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(self.data[index][0]), torch.tensor(self.data[index][1])\n",
    "\n",
    "\n",
    "class ValDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        train_path = data_dir + \"/train.json\"\n",
    "\n",
    "        with open(train_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            data_len = len(data)\n",
    "            self.data = data[int(data_len * split):]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(self.data[index][0]), torch.tensor(self.data[index][1])\n",
    "    \n",
    "    \n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        test_path = data_dir + \"/test.json\"\n",
    "\n",
    "        with open(test_path, 'r') as f:\n",
    "            self.data = json.load(f)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(self.data[index][0]), torch.tensor(self.data[index][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15176\n",
      "3794\n",
      "4743\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "train_dataset = TrainDataset(data_dir='.')\n",
    "val_dataset = ValDataset(data_dir='.')\n",
    "test_dataset = TestDataset(data_dir='.')\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(len(val_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class HeadEstimator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HeadEstimator, self).__init__()\n",
    "        self.fc1 = nn.Linear(24, 64)  # 12 MPII/COCO keypoints * 2 (x, y)\n",
    "        self.fc2 = nn.Linear(64, 512)\n",
    "        self.fc3 = nn.Linear(512, 512)\n",
    "        self.fc4 = nn.Linear(512, 10)  # 17 COCO keypoints * 2 (x, y)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "        self.bn3 = nn.BatchNorm1d(512)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_uniform_(m.weight.data)\n",
    "                nn.init.zeros_(m.bias.data)\n",
    "                \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.bn3(self.fc3(x)))\n",
    "        out = self.fc4(x)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Val Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(trainloader, model, criterion, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch, (inputs, targets) in tqdm(enumerate(trainloader), total=len(trainloader)):\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        #criterion = criterion.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(inputs)\n",
    "        loss = criterion(preds, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.cpu().item()\n",
    "\n",
    "    return total_loss / (batch + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_model(valloader, model, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, (inputs, targets) in tqdm(enumerate(valloader), total = len(valloader)):\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            #criterion = criterion.to(device)\n",
    "\n",
    "            preds = model(inputs)\n",
    "            loss = criterion(preds, targets)\n",
    "            \n",
    "            total_loss += loss.cpu().item()\n",
    "\n",
    "    return total_loss/(batch+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeadEstimator(\n",
      "  (fc1): Linear(in_features=24, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=512, bias=True)\n",
      "  (fc3): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (fc4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "learning_rate = 1e-4\n",
    "epochs = 20\n",
    "\n",
    "trainLoader = DataLoader(train_dataset, batch_size = batch_size, shuffle=True)\n",
    "valLoader = DataLoader(val_dataset, batch_size = batch_size, shuffle=True)\n",
    "\n",
    "model = HeadEstimator()\n",
    "model = model.to(device)\n",
    "\n",
    "def loss(preds, targets):\n",
    "    mse = torch.mean((preds - targets) ** 2)\n",
    "    average_x = torch.mean((torch.mean(preds[:, 1:5], dim=1) - preds[:, 0]) ** 2)\n",
    "    average_y = torch.mean((torch.mean(preds[:, 6:10], dim=1) - preds[:, 5]) ** 2)\n",
    "\n",
    "    return mse + average_x + average_y\n",
    "\n",
    "#criterion = nn.MSELoss()\n",
    "criterion = loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = StepLR(optimizer, step_size=1000, gamma=0.5)\n",
    "\n",
    "history = {'train_loss':[], 'val_loss':[]}\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [00:07<00:00, 16.77it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 37.88it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Train Loss: 1.6683, Val Loss: 0.2577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [00:03<00:00, 31.69it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 139.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20, Train Loss: 0.8325, Val Loss: 0.2005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [00:03<00:00, 32.21it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 142.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20, Train Loss: 0.6310, Val Loss: 0.1538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [00:03<00:00, 32.33it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 140.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20, Train Loss: 0.5113, Val Loss: 0.1456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [00:03<00:00, 32.05it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 142.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20, Train Loss: 0.4440, Val Loss: 0.1163\n",
      "Saving checkpoint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [00:03<00:00, 32.07it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 138.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20, Train Loss: 0.3850, Val Loss: 0.1182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [00:03<00:00, 32.18it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 140.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20, Train Loss: 0.3413, Val Loss: 0.1033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [00:03<00:00, 32.58it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 142.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20, Train Loss: 0.3057, Val Loss: 0.0983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [00:03<00:00, 30.88it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 140.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20, Train Loss: 0.2772, Val Loss: 0.0915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [00:03<00:00, 31.79it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 141.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20, Train Loss: 0.2591, Val Loss: 0.0865\n",
      "Saving checkpoint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [00:03<00:00, 32.38it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 141.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20, Train Loss: 0.2405, Val Loss: 0.0896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [00:03<00:00, 31.94it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 137.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20, Train Loss: 0.2301, Val Loss: 0.0824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [00:03<00:00, 31.06it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 139.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20, Train Loss: 0.2173, Val Loss: 0.0807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [00:03<00:00, 32.21it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 141.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20, Train Loss: 0.2084, Val Loss: 0.0800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [00:03<00:00, 31.71it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 138.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20, Train Loss: 0.1970, Val Loss: 0.0757\n",
      "Saving checkpoint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [00:03<00:00, 31.53it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 144.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20, Train Loss: 0.1888, Val Loss: 0.0769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [00:03<00:00, 31.96it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 145.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20, Train Loss: 0.1778, Val Loss: 0.0762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [00:03<00:00, 32.12it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 145.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20, Train Loss: 0.1734, Val Loss: 0.0765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [00:03<00:00, 32.34it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 144.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20, Train Loss: 0.1679, Val Loss: 0.0714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [00:03<00:00, 32.14it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 136.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20, Train Loss: 0.1634, Val Loss: 0.0721\n",
      "Saving checkpoint...\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Training...\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train_model(trainLoader, model, criterion, optimizer, scheduler, device)\n",
    "    val_loss = val_model(valLoader, model, criterion, device)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "\n",
    "    if (epoch+1) % 5 == 0 or epoch == epochs-1:\n",
    "        print(\"Saving checkpoint...\")\n",
    "        torch.save(model.state_dict(), f'./model.pth')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(epochs), history['train_loss'], label='Train Loss', color='red')\n",
    "plt.plot(range(epochs), history['val_loss'], label='Val Loss', color='blue')\n",
    "\n",
    "plt.title('Loss history')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(testloader, model, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, (inputs, targets) in tqdm(enumerate(testloader), total = len(testloader)):\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            preds = model(inputs)\n",
    "            loss = criterion(preds, targets)\n",
    "            \n",
    "            total_loss += loss.cpu().item()\n",
    "\n",
    "    return total_loss/(batch+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38/38 [00:00<00:00, 43.55it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "testLoader = DataLoader(test_dataset, batch_size = batch_size, shuffle=False)\n",
    "\n",
    "print(\"Testing...\")\n",
    "test_loss = test_model(testLoader, model, criterion, device)\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Inference and Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, device, x):\n",
    "    model.eval()\n",
    "    x = x.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = model(x)\n",
    "        \n",
    "    return pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_pose(pose, scale=2):\n",
    "    \n",
    "    keypoints_x = [point[0] for point in pose]\n",
    "    keypoints_y = [point[1] for point in pose]\n",
    "\n",
    "    min_x = min(keypoints_x)\n",
    "    max_x = max(keypoints_x)\n",
    "    min_y = min(keypoints_y)\n",
    "    max_y = max(keypoints_y)\n",
    "    \n",
    "    height = max_y - min_y\n",
    "    width = max_x - min_x\n",
    "\n",
    "    if height > width:\n",
    "        keypoints_x = [((x - min_x) / height - 0.5) * scale for x in keypoints_x]\n",
    "        keypoints_y = [((y - min_y) / height - 0.5) * scale for y in keypoints_y]\n",
    "        original_scale = height\n",
    "    else:\n",
    "        keypoints_x = [((x - min_x) / width - 0.5) * scale for x in keypoints_x]\n",
    "        keypoints_y = [((y - min_y) / width - 0.5) * scale for y in keypoints_y]\n",
    "        original_scale = width\n",
    "\n",
    "    \n",
    "    print('keypoints_x = \"', keypoints_x, '\"')\n",
    "    print('keypoints_y = \"', keypoints_y, '\"\\n')\n",
    "\n",
    "    pose = [(x, y) for x, y in zip(keypoints_x, keypoints_y)]\n",
    "\n",
    "    return pose, min_x, min_y, original_scale, original_scale\n",
    "\n",
    "def denormalize_pose(pose, min_x, min_y, width, height, scale=2):\n",
    "    pose = [((x / scale + 0.5) * width + min_x, (y / scale + 0.5) * height + min_y) for (x, y) in pose]\n",
    "\n",
    "    return pose\n",
    "\n",
    "def resize_pose(pose, resize_width=200, resize_height=200, padding=10, scale=2):\n",
    "    pose, _, _, _, _ = normalize_pose(pose, scale)\n",
    "    pose = [((x / scale + 0.5) * resize_width + padding, (y / scale + 0.5) * resize_height + padding) for (x, y) in pose]\n",
    "\n",
    "    return pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "coco (original):\n",
    "0 : nose\n",
    "1 2 : eyes\n",
    "3 4 : ears\n",
    "5 7 9 : left arm\n",
    "6 8 10: right arm\n",
    "11 13 15: left leg\n",
    "12 14 16: right leg\n",
    "\n",
    "mpii (ours):\n",
    "13 14 15 : left arm\n",
    "12 11 10 : right arm\n",
    "3 4 5 : left leg\n",
    "2 1 0 : right leg\n",
    "7 : neck\n",
    "9 8 : head\n",
    "\n",
    "coco (ours):\n",
    "0 : nose\n",
    "1 : neck\n",
    "2 3 4 : right arm\n",
    "5 6 7 : left arm\n",
    "8 9 10 : right leg\n",
    "11 12 13 : left leg\n",
    "14 15 : eyes\n",
    "16 17 : ears\n",
    "'''\n",
    "\n",
    "def convert_pose(model, device, pose):\n",
    "    # re-order keypoints to match neural network input\n",
    "    pose, min_x, min_y, width, height = normalize_pose(pose)\n",
    "\n",
    "    reordered_pose = [pose[13], pose[12], pose[14], pose[11], pose[15], pose[10], pose[3], pose[2], pose[4], pose[1], pose[5], pose[0]]\n",
    "\n",
    "    reordered_x = [point[0] for point in reordered_pose]\n",
    "    reordered_y = [point[1] for point in reordered_pose]\n",
    "\n",
    "    input_pose = torch.tensor([reordered_x + reordered_y])\n",
    "\n",
    "    # head inference\n",
    "    head = inference(model, device, input_pose)[0].cpu().tolist()\n",
    "    head_points = [(x, y) for x, y in zip(head[:5], head[5:])]\n",
    "\n",
    "    # insert head keypoints to the pose\n",
    "    reordered_pose = [head_points[0], pose[7], pose[12], pose[11], pose[10], pose[13], pose[14], pose[15], pose[2], pose[1], pose[0], pose[3], pose[4], pose[5],\n",
    "                      head_points[1], head_points[2], head_points[3], head_points[4]]\n",
    "    \n",
    "    reordered_pose = denormalize_pose(reordered_pose, min_x, min_y, width, height)\n",
    "\n",
    "    return reordered_pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "mpii_link_pairs = [[0, 1], [1, 2], [2, 6], \n",
    "              [3, 6], [3, 4], [4, 5], \n",
    "              [6, 7], [7,12], [11, 12], \n",
    "              [10, 11], [7, 13], [13, 14],\n",
    "              [14, 15],[7, 8],[8, 9]]\n",
    "\n",
    "mpii_link_color = [(0, 0, 255), (0, 0, 255), (0, 0, 255),\n",
    "              (0, 255, 0), (0, 255, 0), (0, 255, 0),\n",
    "              (0, 255, 255), (0, 0, 255), (0, 0, 255),\n",
    "              (0, 0, 255), (0, 255, 0), (0, 255, 0),\n",
    "              (0, 255, 0), (0, 255, 255), (0, 255, 255)]\n",
    "\n",
    "mpii_point_color = [(255,0,0),(0,255,0),(0,0,255), \n",
    "               (128,0,0), (0,128,0), (0,0,128),\n",
    "               (255, 255, 0),(0,255,255),(255, 0, 255),\n",
    "               (128,128,0),(0, 128, 128),(128,0,128),\n",
    "               (128,255,0),(128,128,128),(255,128,0),\n",
    "               (255,0,128),(255,255,255)]\n",
    "\n",
    "\n",
    "coco_link_pairs = [[0, 1], [1, 2], [2, 3], \n",
    "              [3, 4], [1, 5], [5, 6], \n",
    "              [6, 7], [1, 8], [8, 9], \n",
    "              [9, 10], [1, 11], [11, 12],\n",
    "              [12, 13],[0, 14],[14, 16], [0, 15], [15, 17]]\n",
    "\n",
    "coco_link_color = [(0, 0, 255), (0, 0, 255), (0, 0, 255),\n",
    "              (0, 255, 0), (0, 255, 0), (0, 255, 0),\n",
    "              (0, 255, 255), (0, 0, 255), (0, 0, 255),\n",
    "              (0, 0, 255), (0, 255, 0), (0, 255, 0),\n",
    "              (0, 255, 0), (0, 255, 255), (0, 255, 255), (128, 128, 0), (128, 0, 128)]\n",
    "\n",
    "coco_point_color = [(255,0,0),(0,255,0),(0,0,255), \n",
    "               (128,0,0), (0,128,0), (0,0,128),\n",
    "               (255, 255, 0),(0,255,255),(255, 0, 255),\n",
    "               (128,128,0),(0, 128, 128),(128,0,128),\n",
    "               (128,255,0),(128,128,128),(255,128,0),\n",
    "               (255,0,128),(255,255,255), (128, 128, 0), (128, 0, 128)]\n",
    "\n",
    "'''\n",
    "mpii (ours):\n",
    "2 1 0 : right leg\n",
    "3 4 5 : left leg\n",
    "12 11 10 : right arm\n",
    "13 14 15 : left arm\n",
    "7 : neck\n",
    "9 8 : head\n",
    "\n",
    "coco (ours):\n",
    "0 : nose\n",
    "1 : neck\n",
    "2 3 4 : right arm\n",
    "5 6 7 : left arm\n",
    "8 9 10 : right leg\n",
    "11 12 13 : left leg\n",
    "14 15 : eyes\n",
    "16 17 : ears\n",
    "'''\n",
    "\n",
    "def vis_pose(image_path, pose, link_pairs, link_color, point_color):\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    pose = [(int(x), int(y)) for (x, y) in pose]\n",
    "\n",
    "    for idx, pair in enumerate(link_pairs):\n",
    "        if pose[pair[0]] != (0, 0) and pose[pair[1]] != (0, 0):\n",
    "            cv2.line(image, pose[pair[0]], pose[pair[1]], link_color[idx], 2)\n",
    "\n",
    "    for idx, point in enumerate(pose):\n",
    "        if point != (0, 0):\n",
    "            cv2.putText(image, str(idx), point, cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "            cv2.circle(image, point, 5, point_color[idx], thickness=-1)\n",
    "\n",
    "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def vis_pose_mpii(image_path, pose):\n",
    "    vis_pose(image_path, pose, mpii_link_pairs, mpii_link_color, mpii_point_color)\n",
    "\n",
    "\n",
    "def vis_pose_coco(image_path, pose):\n",
    "    vis_pose(image_path, pose, coco_link_pairs, coco_link_color, coco_point_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def vis_data(model, device, data, image_dir_path='./affordance_data/data'):\n",
    "    data = data.split(' ')\n",
    "    image_path = os.path.join(image_dir_path, data[0])\n",
    "    pose_data = data[1:-1]\n",
    "    pose_data = [eval(x) for x in pose_data]\n",
    "\n",
    "    pose = []\n",
    "    for i in range(0, len(pose_data), 2):\n",
    "        pose.append((pose_data[i], pose_data[i+1]))\n",
    "\n",
    "    vis_pose_mpii(image_path, pose)\n",
    "\n",
    "    pose = convert_pose(model, device, pose)\n",
    "    vis_pose_coco(image_path, pose)\n",
    "    vis_pose_coco(image_path, resize_pose(pose))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Pose Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Checkpoint\n",
    "model = HeadEstimator()\n",
    "model.load_state_dict(torch.load(f'./model.pth'))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = './affordance_data/trainlist.txt'\n",
    "train_data = []\n",
    "with open(train_data_path, 'r') as f:\n",
    "    train_data = list(f.readlines())\n",
    "\n",
    "for data in train_data[1000:]:\n",
    "    vis_data(model, device, data)\n",
    "    input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
